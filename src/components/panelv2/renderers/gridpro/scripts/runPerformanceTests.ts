/**
 * GridPro æ€§èƒ½æµ‹è¯•è„šæœ¬
 * è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å¹¶ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
 */

import { 
  PerformanceBenchmark, 
  createDefaultBenchmarkConfig,
  runQuickBenchmark,
  type BenchmarkResult 
} from '../performance/PerformanceBenchmark'
import { PerformanceOptimizer } from '../performance/PerformanceOptimizer'
import { 
  createPerformanceMonitoringSuite,
  GridPerformanceOptimizer,
  type PerformanceTimer
} from '../utils/performanceUtils'
import { createDefaultGridProConfig } from '../types/gridpro'
import { 
  PERFORMANCE_PRESETS,
  detectOptimalPreset,
  applyPerformancePreset
} from '../performance'
import type { GridProConfig } from '../types/gridpro'

interface TestScenario {
  name: string
  description: string
  itemCounts: number[]
  config: Partial<GridProConfig>
  expectedPerformance: {
    minFPS: number
    maxRenderTime: number
    maxMemoryMB: number
  }
}

/**
 * æ€§èƒ½æµ‹è¯•åœºæ™¯å®šä¹‰
 */
const TEST_SCENARIOS: TestScenario[] = [
  {
    name: 'lightweight',
    description: 'è½»é‡çº§åœºæ™¯ - å°‘é‡é¡¹ç›®',
    itemCounts: [10, 25, 50],
    config: {
      virtualization: { enabled: false },
      animation: { enabled: true, duration: 300 },
      performance: { batchUpdates: false }
    },
    expectedPerformance: {
      minFPS: 55,
      maxRenderTime: 10,
      maxMemoryMB: 20
    }
  },
  {
    name: 'standard',
    description: 'æ ‡å‡†åœºæ™¯ - ä¸­ç­‰æ•°é‡é¡¹ç›®',
    itemCounts: [100, 200, 300],
    config: {
      virtualization: { enabled: false },
      animation: { enabled: true, duration: 250 },
      performance: { batchUpdates: true, batchSize: 20 }
    },
    expectedPerformance: {
      minFPS: 45,
      maxRenderTime: 25,
      maxMemoryMB: 50
    }
  },
  {
    name: 'heavy',
    description: 'é‡è´Ÿè½½åœºæ™¯ - å¤§é‡é¡¹ç›®',
    itemCounts: [500, 1000, 1500],
    config: {
      virtualization: { enabled: true, bufferSize: 50 },
      animation: { enabled: false },
      performance: { 
        batchUpdates: true, 
        batchSize: 50,
        enableObjectPool: true 
      }
    },
    expectedPerformance: {
      minFPS: 30,
      maxRenderTime: 50,
      maxMemoryMB: 100
    }
  },
  {
    name: 'extreme',
    description: 'æé™åœºæ™¯ - è¶…å¤§é‡é¡¹ç›®',
    itemCounts: [2000, 3000, 5000],
    config: {
      virtualization: { enabled: true, bufferSize: 100 },
      animation: { enabled: false },
      performance: {
        batchUpdates: true,
        batchSize: 100,
        enableObjectPool: true,
        poolSize: 200,
        throttleInterval: 32
      }
    },
    expectedPerformance: {
      minFPS: 20,
      maxRenderTime: 100,
      maxMemoryMB: 200
    }
  }
]

/**
 * æ€§èƒ½æµ‹è¯•ç®¡ç†å™¨
 */
export class PerformanceTestManager {
  private benchmark: PerformanceBenchmark
  private optimizer: PerformanceOptimizer
  private monitoringSuite: ReturnType<typeof createPerformanceMonitoringSuite>
  private results: Map<string, BenchmarkResult[]> = new Map()
  private reports: string[] = []

  constructor() {
    this.benchmark = new PerformanceBenchmark(createDefaultBenchmarkConfig())
    this.optimizer = new PerformanceOptimizer()
    this.monitoringSuite = createPerformanceMonitoringSuite()
  }

  /**
   * è¿è¡Œæ‰€æœ‰æµ‹è¯•åœºæ™¯
   */
  async runAllScenarios(): Promise<{
    results: Map<string, BenchmarkResult[]>
    reports: string[]
    summary: string
  }> {
    console.log('ğŸš€ å¼€å§‹GridProæ€§èƒ½æµ‹è¯•å¥—ä»¶')
    console.log('=' .repeat(60))

    this.results.clear()
    this.reports = []

    // æ£€æµ‹æœ€ä¼˜é¢„è®¾é…ç½®
    const optimalPreset = detectOptimalPreset()
    console.log(`ğŸ¯ æ£€æµ‹åˆ°æœ€ä¼˜é¢„è®¾: ${optimalPreset}`)

    // è¿è¡Œå„ä¸ªåœºæ™¯æµ‹è¯•
    for (const scenario of TEST_SCENARIOS) {
      await this.runScenario(scenario, optimalPreset)
    }

    // ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    const summary = this.generateSummaryReport()

    return {
      results: this.results,
      reports: this.reports,
      summary
    }
  }

  /**
   * è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯
   */
  private async runScenario(
    scenario: TestScenario,
    basePreset: keyof typeof PERFORMANCE_PRESETS
  ): Promise<void> {
    console.log(`\nğŸ“Š è¿è¡Œåœºæ™¯: ${scenario.name}`)
    console.log(`æè¿°: ${scenario.description}`)
    console.log(`é¡¹ç›®æ•°é‡: ${scenario.itemCounts.join(', ')}`)

    // åˆå¹¶é…ç½®
    const baseConfig = applyPerformancePreset(basePreset, createDefaultGridProConfig())
    const scenarioConfig = { ...baseConfig, ...scenario.config }

    // è¿è¡ŒåŸºå‡†æµ‹è¯•
    const scenarioResults: BenchmarkResult[] = []

    for (const itemCount of scenario.itemCounts) {
      console.log(`  æµ‹è¯• ${itemCount} ä¸ªé¡¹ç›®...`)
      
      try {
        // æ›´æ–°é…ç½®ä¸­çš„é¡¹ç›®æ•°é‡ç›¸å…³è®¾ç½®
        const testConfig = this.adaptConfigForItemCount(scenarioConfig, itemCount)
        
        // è¿è¡Œæµ‹è¯•
        const results = await this.runBenchmarkForConfig(testConfig, itemCount)
        scenarioResults.push(...results)
        
        // æ˜¾ç¤ºå³æ—¶ç»“æœ
        const latestResult = results[results.length - 1]
        if (latestResult) {
          console.log(`    ç»¼åˆè¯„åˆ†: ${latestResult.scores.overall}/100`)
          console.log(`    æ¸²æŸ“æ—¶é—´: ${latestResult.renderTime.toFixed(1)}ms`)
          console.log(`    FPS: ${latestResult.fps.toFixed(1)}`)
          console.log(`    å†…å­˜: ${(latestResult.memoryUsage.used / 1024 / 1024).toFixed(1)}MB`)
        }

      } catch (error) {
        console.error(`    æµ‹è¯•å¤±è´¥:`, error)
      }
    }

    // ä¿å­˜ç»“æœ
    this.results.set(scenario.name, scenarioResults)

    // ç”Ÿæˆåœºæ™¯æŠ¥å‘Š
    const scenarioReport = this.generateScenarioReport(scenario, scenarioResults)
    this.reports.push(scenarioReport)

    console.log(`âœ… åœºæ™¯ ${scenario.name} æµ‹è¯•å®Œæˆ`)
  }

  /**
   * ä¸ºç‰¹å®šé¡¹ç›®æ•°é‡è°ƒæ•´é…ç½®
   */
  private adaptConfigForItemCount(config: GridProConfig, itemCount: number): GridProConfig {
    const adaptedConfig = { ...config }

    // è‡ªåŠ¨å¯ç”¨è™šæ‹ŸåŒ–
    if (itemCount > 200 && !adaptedConfig.virtualization?.enabled) {
      adaptedConfig.virtualization = {
        enabled: true,
        bufferSize: Math.min(100, Math.max(20, Math.floor(itemCount / 20))),
        preloadCount: Math.min(20, Math.max(5, Math.floor(itemCount / 100)))
      }
    }

    // è°ƒæ•´æ‰¹å¤„ç†å¤§å°
    if (adaptedConfig.performance?.batchUpdates) {
      adaptedConfig.performance.batchSize = Math.min(100, Math.max(10, Math.floor(itemCount / 20)))
    }

    // è°ƒæ•´å¯¹è±¡æ± å¤§å°
    if (adaptedConfig.performance?.enableObjectPool) {
      adaptedConfig.performance.poolSize = Math.min(500, Math.max(50, Math.floor(itemCount / 10)))
    }

    return adaptedConfig
  }

  /**
   * ä¸ºç‰¹å®šé…ç½®è¿è¡ŒåŸºå‡†æµ‹è¯•
   */
  private async runBenchmarkForConfig(
    config: GridProConfig,
    itemCount: number
  ): Promise<BenchmarkResult[]> {
    // åˆ›å»ºä¸“é—¨çš„åŸºå‡†æµ‹è¯•é…ç½®
    const benchmarkConfig = {
      itemCounts: [itemCount],
      testDuration: 1000, // 1ç§’æµ‹è¯•
      enableMemoryProfiling: true,
      enableFPSMeasurement: true,
      enableInteractionTesting: itemCount <= 500, // å¤§é‡é¡¹ç›®æ—¶è·³è¿‡äº¤äº’æµ‹è¯•
      warmupRounds: 1
    }

    const benchmark = new PerformanceBenchmark(benchmarkConfig)
    
    try {
      const results = await benchmark.runBenchmarkSuite()
      return results
    } finally {
      benchmark.dispose()
    }
  }

  /**
   * ç”Ÿæˆåœºæ™¯æŠ¥å‘Š
   */
  private generateScenarioReport(
    scenario: TestScenario,
    results: BenchmarkResult[]
  ): string {
    let report = `\nğŸ“ˆ ${scenario.name.toUpperCase()} åœºæ™¯æµ‹è¯•æŠ¥å‘Š\n`
    report += '=' .repeat(50) + '\n'
    report += `æè¿°: ${scenario.description}\n`

    if (results.length === 0) {
      report += 'âš ï¸  æ— æµ‹è¯•ç»“æœ\n'
      return report
    }

    // è®¡ç®—å¹³å‡æŒ‡æ ‡
    const avgScore = results.reduce((sum, r) => sum + r.scores.overall, 0) / results.length
    const avgRenderTime = results.reduce((sum, r) => sum + r.renderTime, 0) / results.length
    const avgFPS = results.reduce((sum, r) => sum + r.fps, 0) / results.length
    const avgMemory = results.reduce((sum, r) => sum + r.memoryUsage.used, 0) / results.length

    report += `\nğŸ“Š å¹³å‡æ€§èƒ½æŒ‡æ ‡:\n`
    report += `  ç»¼åˆè¯„åˆ†: ${avgScore.toFixed(1)}/100\n`
    report += `  å¹³å‡æ¸²æŸ“æ—¶é—´: ${avgRenderTime.toFixed(1)}ms\n`
    report += `  å¹³å‡FPS: ${avgFPS.toFixed(1)}\n`
    report += `  å¹³å‡å†…å­˜ä½¿ç”¨: ${(avgMemory / 1024 / 1024).toFixed(1)}MB\n`

    // æ€§èƒ½ç­‰çº§è¯„ä¼°
    const performanceGrade = this.calculatePerformanceGrade(avgScore, scenario.expectedPerformance)
    report += `\nğŸ† æ€§èƒ½ç­‰çº§: ${performanceGrade.grade} (${performanceGrade.description})\n`

    // ä¸é¢„æœŸæ€§èƒ½å¯¹æ¯”
    report += `\nğŸ“‹ æ€§èƒ½å¯¹æ¯”:\n`
    report += `  FPS: ${avgFPS.toFixed(1)} (é¢„æœŸ â‰¥${scenario.expectedPerformance.minFPS})\n`
    report += `  æ¸²æŸ“æ—¶é—´: ${avgRenderTime.toFixed(1)}ms (é¢„æœŸ â‰¤${scenario.expectedPerformance.maxRenderTime}ms)\n`
    report += `  å†…å­˜ä½¿ç”¨: ${(avgMemory / 1024 / 1024).toFixed(1)}MB (é¢„æœŸ â‰¤${scenario.expectedPerformance.maxMemoryMB}MB)\n`

    // ç”Ÿæˆä¼˜åŒ–å»ºè®®
    const suggestions = this.optimizer.analyzePerformance(results, scenario.config as GridProConfig)
    if (suggestions.length > 0) {
      report += `\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\n`
      suggestions.slice(0, 3).forEach((suggestion, index) => {
        report += `  ${index + 1}. ${suggestion.rule.description}\n`
        report += `     ${suggestion.reason}\n`
      })
    }

    return report
  }

  /**
   * è®¡ç®—æ€§èƒ½ç­‰çº§
   */
  private calculatePerformanceGrade(
    avgScore: number,
    expected: TestScenario['expectedPerformance']
  ): { grade: string; description: string } {
    if (avgScore >= 90) {
      return { grade: 'S', description: 'å“è¶Š' }
    } else if (avgScore >= 80) {
      return { grade: 'A', description: 'ä¼˜ç§€' }
    } else if (avgScore >= 70) {
      return { grade: 'B', description: 'è‰¯å¥½' }
    } else if (avgScore >= 60) {
      return { grade: 'C', description: 'åŠæ ¼' }
    } else {
      return { grade: 'D', description: 'éœ€è¦ä¼˜åŒ–' }
    }
  }

  /**
   * ç”Ÿæˆç»¼åˆæŠ¥å‘Š
   */
  private generateSummaryReport(): string {
    let summary = `\nğŸ† GridPro æ€§èƒ½æµ‹è¯•ç»¼åˆæŠ¥å‘Š\n`
    summary += '=' .repeat(60) + '\n'
    summary += `æµ‹è¯•æ—¶é—´: ${new Date().toLocaleString()}\n`
    summary += `æµ‹è¯•åœºæ™¯: ${TEST_SCENARIOS.length} ä¸ª\n`

    // ç»Ÿè®¡æ‰€æœ‰ç»“æœ
    const allResults: BenchmarkResult[] = []
    this.results.forEach(results => allResults.push(...results))

    if (allResults.length === 0) {
      summary += '\nâš ï¸  æ— æœ‰æ•ˆæµ‹è¯•ç»“æœ\n'
      return summary
    }

    // è®¡ç®—æ€»ä½“æŒ‡æ ‡
    const overallAvgScore = allResults.reduce((sum, r) => sum + r.scores.overall, 0) / allResults.length
    const overallAvgRenderTime = allResults.reduce((sum, r) => sum + r.renderTime, 0) / allResults.length
    const overallAvgFPS = allResults.reduce((sum, r) => sum + r.fps, 0) / allResults.length
    const overallAvgMemory = allResults.reduce((sum, r) => sum + r.memoryUsage.used, 0) / allResults.length

    summary += `\nğŸ“Š æ€»ä½“æ€§èƒ½æŒ‡æ ‡:\n`
    summary += `  ç»¼åˆè¯„åˆ†: ${overallAvgScore.toFixed(1)}/100\n`
    summary += `  å¹³å‡æ¸²æŸ“æ—¶é—´: ${overallAvgRenderTime.toFixed(1)}ms\n`
    summary += `  å¹³å‡FPS: ${overallAvgFPS.toFixed(1)}\n`
    summary += `  å¹³å‡å†…å­˜ä½¿ç”¨: ${(overallAvgMemory / 1024 / 1024).toFixed(1)}MB\n`

    // å„åœºæ™¯è¡¨ç°æ€»ç»“
    summary += `\nğŸ“ˆ å„åœºæ™¯è¡¨ç°:\n`
    this.results.forEach((results, scenarioName) => {
      if (results.length > 0) {
        const scenarioAvgScore = results.reduce((sum, r) => sum + r.scores.overall, 0) / results.length
        const grade = this.calculatePerformanceGrade(scenarioAvgScore, { minFPS: 30, maxRenderTime: 50, maxMemoryMB: 100 })
        summary += `  ${scenarioName}: ${scenarioAvgScore.toFixed(1)}/100 (${grade.grade})\n`
      }
    })

    // æ€§èƒ½å»ºè®®
    summary += `\nğŸ’¡ æ€»ä½“å»ºè®®:\n`
    if (overallAvgScore >= 80) {
      summary += `  âœ… GridProæ¸²æŸ“å™¨æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨\n`
    } else if (overallAvgScore >= 60) {
      summary += `  âš ï¸  æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®æ ¹æ®å…·ä½“åœºæ™¯è¿›è¡Œä¼˜åŒ–\n`
    } else {
      summary += `  âŒ æ€§èƒ½éœ€è¦æ”¹è¿›ï¼Œå»ºè®®å¯ç”¨æ‰€æœ‰ä¼˜åŒ–é€‰é¡¹\n`
    }

    // æœ€ä½³å®è·µå»ºè®®
    summary += `\nğŸ¯ æœ€ä½³å®è·µå»ºè®®:\n`
    summary += `  1. å½“é¡¹ç›®æ•°é‡ >200 æ—¶ï¼Œå¯ç”¨è™šæ‹ŸåŒ–\n`
    summary += `  2. åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šç¦ç”¨åŠ¨ç”»ä»¥æé«˜æ€§èƒ½\n`
    summary += `  3. å¯ç”¨æ‰¹é‡æ›´æ–°å’Œå¯¹è±¡æ± ä¼˜åŒ–å†…å­˜ä½¿ç”¨\n`
    summary += `  4. æ ¹æ®è®¾å¤‡èƒ½åŠ›è‡ªåŠ¨é€‰æ‹©æ€§èƒ½é¢„è®¾\n`
    summary += `  5. å®šæœŸè¿è¡Œæ€§èƒ½æµ‹è¯•ï¼Œç›‘æ§æ€§èƒ½å›å½’\n`

    return summary
  }

  /**
   * æ¸…ç†èµ„æº
   */
  dispose(): void {
    this.benchmark.dispose()
    this.monitoringSuite.fps.stop()
  }
}

/**
 * è¿è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•
 */
export async function runQuickPerformanceTest(): Promise<string> {
  console.log('ğŸš€ è¿è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•...')
  
  try {
    const results = await runQuickBenchmark()
    
    let report = '\nâš¡ å¿«é€Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n'
    report += '=' .repeat(40) + '\n'
    
    if (results.length > 0) {
      const avgScore = results.reduce((sum, r) => sum + r.scores.overall, 0) / results.length
      report += `ç»¼åˆè¯„åˆ†: ${avgScore.toFixed(1)}/100\n`
      
      if (avgScore >= 70) {
        report += 'âœ… æ€§èƒ½è¡¨ç°è‰¯å¥½\n'
      } else {
        report += 'âš ï¸  å»ºè®®è¿è¡Œå®Œæ•´æµ‹è¯•è·å–ä¼˜åŒ–å»ºè®®\n'
      }
    } else {
      report += 'âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®\n'
    }
    
    return report
    
  } catch (error) {
    return `âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥: ${error}`
  }
}

/**
 * è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•
 */
export async function runFullPerformanceTest(): Promise<{
  success: boolean
  summary: string
  reports: string[]
  rawData: Map<string, BenchmarkResult[]>
}> {
  const testManager = new PerformanceTestManager()
  
  try {
    const { results, reports, summary } = await testManager.runAllScenarios()
    
    return {
      success: true,
      summary,
      reports,
      rawData: results
    }
    
  } catch (error) {
    return {
      success: false,
      summary: `æµ‹è¯•å¤±è´¥: ${error}`,
      reports: [],
      rawData: new Map()
    }
  } finally {
    testManager.dispose()
  }
}

/**
 * å¯¼å‡ºæµ‹è¯•ç»“æœåˆ°æ–‡ä»¶
 */
export function exportTestResults(
  results: Map<string, BenchmarkResult[]>,
  summary: string,
  format: 'json' | 'csv' | 'markdown' = 'json'
): string {
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-')

  switch (format) {
    case 'json':
      return JSON.stringify({
        timestamp,
        summary,
        results: Object.fromEntries(results),
        metadata: {
          browser: navigator.userAgent,
          memory: (navigator as any).deviceMemory || 'unknown',
          cores: navigator.hardwareConcurrency || 'unknown'
        }
      }, null, 2)

    case 'csv':
      let csv = 'Scenario,TestName,ItemCount,OverallScore,RenderingScore,MemoryScore,ResponsivenessScore,RenderTime,FPS,MemoryUsed\n'
      results.forEach((scenarioResults, scenarioName) => {
        scenarioResults.forEach(result => {
          const itemCount = result.testName.match(/(\d+)/) || ['', '0']
          csv += `${scenarioName},${result.testName},${itemCount[1]},${result.scores.overall},${result.scores.rendering},${result.scores.memory},${result.scores.responsiveness},${result.renderTime},${result.fps},${result.memoryUsage.used}\n`
        })
      })
      return csv

    case 'markdown':
      let md = `# GridPro æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n\n`
      md += `**æµ‹è¯•æ—¶é—´:** ${new Date().toLocaleString()}\n\n`
      md += `## ç»¼åˆæŠ¥å‘Š\n\n${summary}\n\n`
      md += `## è¯¦ç»†ç»“æœ\n\n`
      
      results.forEach((scenarioResults, scenarioName) => {
        md += `### ${scenarioName.toUpperCase()}\n\n`
        md += `| æµ‹è¯•åç§° | ç»¼åˆè¯„åˆ† | æ¸²æŸ“æ—¶é—´(ms) | FPS | å†…å­˜ä½¿ç”¨(MB) |\n`
        md += `|---------|---------|-------------|-----|-------------|\n`
        
        scenarioResults.forEach(result => {
          md += `| ${result.testName} | ${result.scores.overall} | ${result.renderTime.toFixed(1)} | ${result.fps.toFixed(1)} | ${(result.memoryUsage.used / 1024 / 1024).toFixed(1)} |\n`
        })
        
        md += `\n`
      })
      
      return md

    default:
      return ''
  }
}

// å¦‚æœåœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ç›´æ¥è¿è¡Œ
if (typeof window !== 'undefined') {
  // æ·»åŠ åˆ°å…¨å±€å¯¹è±¡ä¾›è°ƒè¯•ä½¿ç”¨
  ;(window as any).GridProPerformanceTest = {
    runQuickTest: runQuickPerformanceTest,
    runFullTest: runFullPerformanceTest,
    exportResults: exportTestResults
  }
}